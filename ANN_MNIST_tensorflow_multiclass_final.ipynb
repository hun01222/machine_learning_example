{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ANN_MNIST_tensorflow_multiclass_final.ipynb","provenance":[],"authorship_tag":"ABX9TyOodFd4rTRP9rCV5JFY4VrG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAhAs1__A2bf","executionInfo":{"status":"ok","timestamp":1639620898107,"user_tz":-540,"elapsed":122458,"user":{"displayName":"박지훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6-Ag58_xLqlUgLlxTrvTfpPDdLrq6GdSW3nh4Kw=s64","userId":"08329669712993464717"}},"outputId":"6744fe0b-85db-448b-98f0-1d2781631e65"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n","Epoch_A: 1 \t Step_A: 10 \t Accuracy_A: 0.9193999767303467\n","Epoch_A: 1 \t Step_A: 20 \t Accuracy_A: 0.9508000016212463\n","Epoch_A: 1 \t Step_A: 30 \t Accuracy_A: 0.9539999961853027\n","Epoch_A: 1 \t Step_A: 40 \t Accuracy_A: 0.9628999829292297\n","Epoch_A: 1 \t Step_A: 50 \t Accuracy_A: 0.9661999940872192\n","Epoch_A: 1 \t Step_A: 60 \t Accuracy_A: 0.9736999869346619\n","Epoch_A: 2 \t Step_A: 10 \t Accuracy_A: 0.9793000221252441\n","Epoch_A: 2 \t Step_A: 20 \t Accuracy_A: 0.9785000085830688\n","Epoch_A: 2 \t Step_A: 30 \t Accuracy_A: 0.9818000197410583\n","Epoch_A: 2 \t Step_A: 40 \t Accuracy_A: 0.9818999767303467\n","Epoch_A: 2 \t Step_A: 50 \t Accuracy_A: 0.9847000241279602\n","Epoch_A: 2 \t Step_A: 60 \t Accuracy_A: 0.986299991607666\n","Epoch_A: 3 \t Step_A: 10 \t Accuracy_A: 0.9861000180244446\n","Epoch_A: 3 \t Step_A: 20 \t Accuracy_A: 0.986299991607666\n","Epoch_A: 3 \t Step_A: 30 \t Accuracy_A: 0.9907000064849854\n","Epoch_A: 3 \t Step_A: 40 \t Accuracy_A: 0.9904000163078308\n","Epoch_A: 3 \t Step_A: 50 \t Accuracy_A: 0.9919000267982483\n","Epoch_A: 3 \t Step_A: 60 \t Accuracy_A: 0.9933000206947327\n","Epoch_A: 4 \t Step_A: 10 \t Accuracy_A: 0.9940000176429749\n","Epoch_A: 4 \t Step_A: 20 \t Accuracy_A: 0.9909999966621399\n","Epoch_A: 4 \t Step_A: 30 \t Accuracy_A: 0.9939000010490417\n","Epoch_A: 4 \t Step_A: 40 \t Accuracy_A: 0.9926000237464905\n","Epoch_A: 4 \t Step_A: 50 \t Accuracy_A: 0.993399977684021\n","Epoch_A: 4 \t Step_A: 60 \t Accuracy_A: 0.9947999715805054\n","Epoch_A: 5 \t Step_A: 10 \t Accuracy_A: 0.9937999844551086\n","Epoch_A: 5 \t Step_A: 20 \t Accuracy_A: 0.9958000183105469\n","Epoch_A: 5 \t Step_A: 30 \t Accuracy_A: 0.9958000183105469\n","Epoch_A: 5 \t Step_A: 40 \t Accuracy_A: 0.9944000244140625\n","Epoch_A: 5 \t Step_A: 50 \t Accuracy_A: 0.9965000152587891\n","Epoch_A: 5 \t Step_A: 60 \t Accuracy_A: 0.9973000288009644\n","Epoch_A: 6 \t Step_A: 10 \t Accuracy_A: 0.995199978351593\n","Epoch_A: 6 \t Step_A: 20 \t Accuracy_A: 0.9961000084877014\n","Epoch_A: 6 \t Step_A: 30 \t Accuracy_A: 0.9968000054359436\n","Epoch_A: 6 \t Step_A: 40 \t Accuracy_A: 0.996399998664856\n","Epoch_A: 6 \t Step_A: 50 \t Accuracy_A: 0.9955999851226807\n","Epoch_A: 6 \t Step_A: 60 \t Accuracy_A: 0.996399998664856\n","Epoch_A: 7 \t Step_A: 10 \t Accuracy_A: 0.994700014591217\n","Epoch_A: 7 \t Step_A: 20 \t Accuracy_A: 0.9976999759674072\n","Epoch_A: 7 \t Step_A: 30 \t Accuracy_A: 0.9941999912261963\n","Epoch_A: 7 \t Step_A: 40 \t Accuracy_A: 0.9944000244140625\n","Epoch_A: 7 \t Step_A: 50 \t Accuracy_A: 0.9976000189781189\n","Epoch_A: 7 \t Step_A: 60 \t Accuracy_A: 0.9973999857902527\n","Epoch_A: 8 \t Step_A: 10 \t Accuracy_A: 0.9980000257492065\n","Epoch_A: 8 \t Step_A: 20 \t Accuracy_A: 0.9962000250816345\n","Epoch_A: 8 \t Step_A: 30 \t Accuracy_A: 0.996999979019165\n","Epoch_A: 8 \t Step_A: 40 \t Accuracy_A: 0.998199999332428\n","Epoch_A: 8 \t Step_A: 50 \t Accuracy_A: 0.995199978351593\n","Epoch_A: 8 \t Step_A: 60 \t Accuracy_A: 0.9980999827384949\n","Epoch_A: 9 \t Step_A: 10 \t Accuracy_A: 0.996999979019165\n","Epoch_A: 9 \t Step_A: 20 \t Accuracy_A: 0.9962000250816345\n","Epoch_A: 9 \t Step_A: 30 \t Accuracy_A: 0.9980000257492065\n","Epoch_A: 9 \t Step_A: 40 \t Accuracy_A: 0.9972000122070312\n","Epoch_A: 9 \t Step_A: 50 \t Accuracy_A: 0.9973999857902527\n","Epoch_A: 9 \t Step_A: 60 \t Accuracy_A: 0.9976000189781189\n","Epoch_A: 10 \t Step_A: 10 \t Accuracy_A: 0.9965999722480774\n","Epoch_A: 10 \t Step_A: 20 \t Accuracy_A: 0.9969000220298767\n","Epoch_A: 10 \t Step_A: 30 \t Accuracy_A: 0.995199978351593\n","Epoch_A: 10 \t Step_A: 40 \t Accuracy_A: 0.9973000288009644\n","Epoch_A: 10 \t Step_A: 50 \t Accuracy_A: 0.9988999962806702\n","Epoch_A: 10 \t Step_A: 60 \t Accuracy_A: 0.9968000054359436\n","Epoch_B: 1 \t Step_B: 10 \t Accuracy_B: 0.9246000051498413\n","Epoch_B: 1 \t Step_B: 20 \t Accuracy_B: 0.9485999941825867\n","Epoch_B: 1 \t Step_B: 30 \t Accuracy_B: 0.9592000246047974\n","Epoch_B: 1 \t Step_B: 40 \t Accuracy_B: 0.9648000001907349\n","Epoch_B: 1 \t Step_B: 50 \t Accuracy_B: 0.9718999862670898\n","Epoch_B: 1 \t Step_B: 60 \t Accuracy_B: 0.9782999753952026\n","Epoch_B: 2 \t Step_B: 10 \t Accuracy_B: 0.9763000011444092\n","Epoch_B: 2 \t Step_B: 20 \t Accuracy_B: 0.9817000031471252\n","Epoch_B: 2 \t Step_B: 30 \t Accuracy_B: 0.9817000031471252\n","Epoch_B: 2 \t Step_B: 40 \t Accuracy_B: 0.9830999970436096\n","Epoch_B: 2 \t Step_B: 50 \t Accuracy_B: 0.9847999811172485\n","Epoch_B: 2 \t Step_B: 60 \t Accuracy_B: 0.9901999831199646\n","Epoch_B: 3 \t Step_B: 10 \t Accuracy_B: 0.9876000285148621\n","Epoch_B: 3 \t Step_B: 20 \t Accuracy_B: 0.9911999702453613\n","Epoch_B: 3 \t Step_B: 30 \t Accuracy_B: 0.991100013256073\n","Epoch_B: 3 \t Step_B: 40 \t Accuracy_B: 0.9921000003814697\n","Epoch_B: 3 \t Step_B: 50 \t Accuracy_B: 0.9897000193595886\n","Epoch_B: 3 \t Step_B: 60 \t Accuracy_B: 0.9930999875068665\n","Epoch_B: 4 \t Step_B: 10 \t Accuracy_B: 0.9916999936103821\n","Epoch_B: 4 \t Step_B: 20 \t Accuracy_B: 0.992900013923645\n","Epoch_B: 4 \t Step_B: 30 \t Accuracy_B: 0.993399977684021\n","Epoch_B: 4 \t Step_B: 40 \t Accuracy_B: 0.9948999881744385\n","Epoch_B: 4 \t Step_B: 50 \t Accuracy_B: 0.9926999807357788\n","Epoch_B: 4 \t Step_B: 60 \t Accuracy_B: 0.993399977684021\n","Epoch_B: 5 \t Step_B: 10 \t Accuracy_B: 0.9944000244140625\n","Epoch_B: 5 \t Step_B: 20 \t Accuracy_B: 0.9930999875068665\n","Epoch_B: 5 \t Step_B: 30 \t Accuracy_B: 0.9951000213623047\n","Epoch_B: 5 \t Step_B: 40 \t Accuracy_B: 0.9937999844551086\n","Epoch_B: 5 \t Step_B: 50 \t Accuracy_B: 0.9955999851226807\n","Epoch_B: 5 \t Step_B: 60 \t Accuracy_B: 0.9954000115394592\n","Epoch_B: 6 \t Step_B: 10 \t Accuracy_B: 0.9937000274658203\n","Epoch_B: 6 \t Step_B: 20 \t Accuracy_B: 0.9961000084877014\n","Epoch_B: 6 \t Step_B: 30 \t Accuracy_B: 0.9952999949455261\n","Epoch_B: 6 \t Step_B: 40 \t Accuracy_B: 0.9936000108718872\n","Epoch_B: 6 \t Step_B: 50 \t Accuracy_B: 0.9951000213623047\n","Epoch_B: 6 \t Step_B: 60 \t Accuracy_B: 0.9972000122070312\n","Epoch_B: 7 \t Step_B: 10 \t Accuracy_B: 0.9961000084877014\n","Epoch_B: 7 \t Step_B: 20 \t Accuracy_B: 0.9976999759674072\n","Epoch_B: 7 \t Step_B: 30 \t Accuracy_B: 0.9944000244140625\n","Epoch_B: 7 \t Step_B: 40 \t Accuracy_B: 0.9951000213623047\n","Epoch_B: 7 \t Step_B: 50 \t Accuracy_B: 0.9940999746322632\n","Epoch_B: 7 \t Step_B: 60 \t Accuracy_B: 0.9952999949455261\n","Epoch_B: 8 \t Step_B: 10 \t Accuracy_B: 0.9969000220298767\n","Epoch_B: 8 \t Step_B: 20 \t Accuracy_B: 0.9959999918937683\n","Epoch_B: 8 \t Step_B: 30 \t Accuracy_B: 0.9962000250816345\n","Epoch_B: 8 \t Step_B: 40 \t Accuracy_B: 0.9966999888420105\n","Epoch_B: 8 \t Step_B: 50 \t Accuracy_B: 0.9965999722480774\n","Epoch_B: 8 \t Step_B: 60 \t Accuracy_B: 0.9988999962806702\n","Epoch_B: 9 \t Step_B: 10 \t Accuracy_B: 0.9965000152587891\n","Epoch_B: 9 \t Step_B: 20 \t Accuracy_B: 0.9976000189781189\n","Epoch_B: 9 \t Step_B: 30 \t Accuracy_B: 0.9987999796867371\n","Epoch_B: 9 \t Step_B: 40 \t Accuracy_B: 0.9972000122070312\n","Epoch_B: 9 \t Step_B: 50 \t Accuracy_B: 0.9970999956130981\n","Epoch_B: 9 \t Step_B: 60 \t Accuracy_B: 0.9986000061035156\n","Epoch_B: 10 \t Step_B: 10 \t Accuracy_B: 0.9980999827384949\n","Epoch_B: 10 \t Step_B: 20 \t Accuracy_B: 0.9976000189781189\n","Epoch_B: 10 \t Step_B: 30 \t Accuracy_B: 0.9955999851226807\n","Epoch_B: 10 \t Step_B: 40 \t Accuracy_B: 0.9991999864578247\n","Epoch_B: 10 \t Step_B: 50 \t Accuracy_B: 0.9984999895095825\n","Epoch_B: 10 \t Step_B: 60 \t Accuracy_B: 0.998199999332428\n","Testing_A\n","Test Accuracy_A: 0.9982 \n","\n","Testing_B\n","Test Accuracy_B: 0.9982\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.datasets import mnist\n","import torch.nn as nn\n","\n","(train_images, train_labels),(test_images, test_labels)=mnist.load_data()\n","print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n","train_images=train_images.reshape([-1,784]).astype('float32')/255.0\n","test_images=test_images.reshape([-1,784]).astype('float32')/255.0\n","train_labels=tf.one_hot(train_labels, depth=10) # size 10짜리 one-hot encoding 만듦\n","test_labels=tf.one_hot(test_labels, depth=10)\n","batch_size=1000\n","train_data=tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n","train_data=train_data.repeat().shuffle(5000).batch(batch_size) # 데이터를 shuffle 하여야 고른 분포를 가짐\n","\n","nH1=256\n","nH2=256\n","nH3=256\n","\n","class class_SoftmaxNN_LC_A(nn.Module): # class 는 항상 pytorch로 만들었기 때문\n","  def __init__(self):\n","    self.W_1=tf.Variable(tf.random.normal([784,nH1]))\n","    self.b_1=tf.Variable(tf.random.normal([nH1]))\n","    self.W_2=tf.Variable(tf.random.normal([nH1,nH2]))\n","    self.b_2=tf.Variable(tf.random.normal([nH2]))\n","    self.W_3=tf.Variable(tf.random.normal([nH2,nH3]))\n","    self.b_3=tf.Variable(tf.random.normal([nH3]))\n","    self.W_o=tf.Variable(tf.random.normal([nH3,10]))\n","    self.b_o=tf.Variable(tf.random.normal([10]))\n","  def __call__(self,x):\n","    H_1=tf.nn.relu(tf.matmul(x,self.W_1)+self.b_1)\n","    H_2=tf.nn.relu(tf.matmul(H_1,self.W_2)+self.b_2)\n","    H_3=tf.nn.relu(tf.matmul(H_2,self.W_3)+self.b_3)\n","    Out=tf.matmul(H_3,self.W_o)+self.b_o\n","    return Out\n","\n","class class_SoftmaxNN_LC_B(nn.Module): # class 는 항상 pytorch로 만들었기 때문\n","  def __init__(self):\n","    self.W_1=tf.Variable(tf.random.normal([784,nH1]))\n","    self.b_1=tf.Variable(tf.random.normal([nH1]))\n","    self.W_2=tf.Variable(tf.random.normal([nH1,nH2]))\n","    self.b_2=tf.Variable(tf.random.normal([nH2]))\n","    self.W_3=tf.Variable(tf.random.normal([nH2,nH3]))\n","    self.b_3=tf.Variable(tf.random.normal([nH3]))\n","    self.W_o=tf.Variable(tf.random.normal([nH3,10]))\n","    self.b_o=tf.Variable(tf.random.normal([10]))\n","  def __call__(self,x):\n","    H_1=tf.nn.relu(tf.matmul(x,self.W_1)+self.b_1)\n","    H_2=tf.nn.relu(tf.matmul(H_1,self.W_2)+self.b_2)\n","    H_3=tf.nn.relu(tf.matmul(H_2,self.W_3)+self.b_3)\n","    Out=tf.matmul(H_3,self.W_o)+self.b_o\n","    return Out\n","\n","model_SoftmaxNN_LC_A=class_SoftmaxNN_LC_A()\n","model_SoftmaxNN_LC_B=class_SoftmaxNN_LC_B()\n","optimizer=tf.optimizers.Adam(0.01)\n","\n","def cost_Softmax(x,y):\n","  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x,labels=y))\n","\n","def train_optimization_A(MNIST_images,MNIST_labels):\n","  with tf.GradientTape() as g:\n","    prediction=model_SoftmaxNN_LC_A(MNIST_images)\n","    cost=cost_Softmax(prediction,MNIST_labels)\n","  gradients=g.gradient(cost,vars(model_SoftmaxNN_LC_A).values())\n","  tf.optimizers.Adam(0.01).apply_gradients(zip(gradients,vars(model_SoftmaxNN_LC_A).values()))\n","def train_optimization_B(MNIST_images,MNIST_labels):\n","  with tf.GradientTape() as g:\n","    prediction=model_SoftmaxNN_LC_B(MNIST_images)\n","    cost=cost_Softmax(prediction,MNIST_labels)\n","  gradients=g.gradient(cost,vars(model_SoftmaxNN_LC_B).values())\n","  tf.optimizers.Adam(0.01).apply_gradients(zip(gradients,vars(model_SoftmaxNN_LC_B).values()))\n","\n","for epoch in range(1,11):\n","  for step, (batch_images,batch_labels) in enumerate(train_data.take(60),1):\n","    train_optimization_A(batch_images,batch_labels)\n","    if step%10==0:\n","      pred=tf.nn.softmax(model_SoftmaxNN_LC_A(batch_images))\n","      accuracy=tf.reduce_mean(tf.cast(tf.equal(pred,batch_labels),tf.float32))\n","      print('Epoch_A: {} \\t Step_A: {} \\t Accuracy_A: {}'.format(epoch,step,accuracy))\n","for epoch in range(1,11):\n","  for step, (batch_images,batch_labels) in enumerate(train_data.take(60),1):\n","    train_optimization_B(batch_images,batch_labels)\n","    if step%10==0:\n","      pred=tf.nn.softmax(model_SoftmaxNN_LC_B(batch_images))\n","      accuracy=tf.reduce_mean(tf.cast(tf.equal(pred,batch_labels),tf.float32))\n","      print('Epoch_B: {} \\t Step_B: {} \\t Accuracy_B: {}'.format(epoch,step,accuracy))\n","\n","print('Testing_A')\n","test_model=tf.nn.softmax(model_SoftmaxNN_LC_A(test_images))\n","test_accuracy=tf.reduce_mean(tf.cast(tf.equal(pred,batch_labels),tf.float32))\n","print('Test Accuracy_A:', test_accuracy.numpy(), '\\n')\n","\n","print('Testing_B')\n","test_model=tf.nn.softmax(model_SoftmaxNN_LC_A(test_images))\n","test_accuracy=tf.reduce_mean(tf.cast(tf.equal(pred,batch_labels),tf.float32))\n","print('Test Accuracy_B:', test_accuracy.numpy())"]}]}