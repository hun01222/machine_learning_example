{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ANN_MNIST_tensorflow_class_final.ipynb","provenance":[],"authorship_tag":"ABX9TyM5+nxR/bM3WYVKKQpxFWiG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KadJlvwq68yn","executionInfo":{"status":"ok","timestamp":1639619450524,"user_tz":-540,"elapsed":214741,"user":{"displayName":"박지훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6-Ag58_xLqlUgLlxTrvTfpPDdLrq6GdSW3nh4Kw=s64","userId":"08329669712993464717"}},"outputId":"ca206543-e090-4865-d2a5-b60fe8d43c31"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n","Epoch: 1 \t Step: 10 \t Accuracy: 0.8917999863624573\n","Epoch: 1 \t Step: 20 \t Accuracy: 0.9458000063896179\n","Epoch: 1 \t Step: 30 \t Accuracy: 0.9531999826431274\n","Epoch: 1 \t Step: 40 \t Accuracy: 0.9656000137329102\n","Epoch: 1 \t Step: 50 \t Accuracy: 0.968999981880188\n","Epoch: 1 \t Step: 60 \t Accuracy: 0.9783999919891357\n","Epoch: 2 \t Step: 10 \t Accuracy: 0.9746000170707703\n","Epoch: 2 \t Step: 20 \t Accuracy: 0.9807999730110168\n","Epoch: 2 \t Step: 30 \t Accuracy: 0.9824000000953674\n","Epoch: 2 \t Step: 40 \t Accuracy: 0.9842000007629395\n","Epoch: 2 \t Step: 50 \t Accuracy: 0.98580002784729\n","Epoch: 2 \t Step: 60 \t Accuracy: 0.9883999824523926\n","Epoch: 3 \t Step: 10 \t Accuracy: 0.991599977016449\n","Epoch: 3 \t Step: 20 \t Accuracy: 0.9846000075340271\n","Epoch: 3 \t Step: 30 \t Accuracy: 0.9911999702453613\n","Epoch: 3 \t Step: 40 \t Accuracy: 0.9887999892234802\n","Epoch: 3 \t Step: 50 \t Accuracy: 0.9890000224113464\n","Epoch: 3 \t Step: 60 \t Accuracy: 0.9914000034332275\n","Epoch: 4 \t Step: 10 \t Accuracy: 0.9894000291824341\n","Epoch: 4 \t Step: 20 \t Accuracy: 0.9932000041007996\n","Epoch: 4 \t Step: 30 \t Accuracy: 0.9937999844551086\n","Epoch: 4 \t Step: 40 \t Accuracy: 0.995199978351593\n","Epoch: 4 \t Step: 50 \t Accuracy: 0.9937999844551086\n","Epoch: 4 \t Step: 60 \t Accuracy: 0.995199978351593\n","Epoch: 5 \t Step: 10 \t Accuracy: 0.9950000047683716\n","Epoch: 5 \t Step: 20 \t Accuracy: 0.9977999925613403\n","Epoch: 5 \t Step: 30 \t Accuracy: 0.9929999709129333\n","Epoch: 5 \t Step: 40 \t Accuracy: 0.9944000244140625\n","Epoch: 5 \t Step: 50 \t Accuracy: 0.9950000047683716\n","Epoch: 5 \t Step: 60 \t Accuracy: 0.995199978351593\n","Epoch: 6 \t Step: 10 \t Accuracy: 0.9926000237464905\n","Epoch: 6 \t Step: 20 \t Accuracy: 0.9973999857902527\n","Epoch: 6 \t Step: 30 \t Accuracy: 0.9927999973297119\n","Epoch: 6 \t Step: 40 \t Accuracy: 0.9926000237464905\n","Epoch: 6 \t Step: 50 \t Accuracy: 0.9948999881744385\n","Epoch: 6 \t Step: 60 \t Accuracy: 0.9965999722480774\n","Epoch: 7 \t Step: 10 \t Accuracy: 0.996999979019165\n","Epoch: 7 \t Step: 20 \t Accuracy: 0.9941999912261963\n","Epoch: 7 \t Step: 30 \t Accuracy: 0.9945999979972839\n","Epoch: 7 \t Step: 40 \t Accuracy: 0.9937999844551086\n","Epoch: 7 \t Step: 50 \t Accuracy: 0.9955999851226807\n","Epoch: 7 \t Step: 60 \t Accuracy: 0.9976000189781189\n","Epoch: 8 \t Step: 10 \t Accuracy: 0.9965999722480774\n","Epoch: 8 \t Step: 20 \t Accuracy: 0.9962000250816345\n","Epoch: 8 \t Step: 30 \t Accuracy: 0.9944000244140625\n","Epoch: 8 \t Step: 40 \t Accuracy: 0.9959999918937683\n","Epoch: 8 \t Step: 50 \t Accuracy: 0.9958000183105469\n","Epoch: 8 \t Step: 60 \t Accuracy: 0.9980000257492065\n","Epoch: 9 \t Step: 10 \t Accuracy: 0.9936000108718872\n","Epoch: 9 \t Step: 20 \t Accuracy: 0.9976000189781189\n","Epoch: 9 \t Step: 30 \t Accuracy: 0.996999979019165\n","Epoch: 9 \t Step: 40 \t Accuracy: 0.996999979019165\n","Epoch: 9 \t Step: 50 \t Accuracy: 0.9958000183105469\n","Epoch: 9 \t Step: 60 \t Accuracy: 0.998199999332428\n","Epoch: 10 \t Step: 10 \t Accuracy: 0.9977999925613403\n","Epoch: 10 \t Step: 20 \t Accuracy: 0.9972000122070312\n","Epoch: 10 \t Step: 30 \t Accuracy: 0.998199999332428\n","Epoch: 10 \t Step: 40 \t Accuracy: 0.998199999332428\n","Epoch: 10 \t Step: 50 \t Accuracy: 0.9959999918937683\n","Epoch: 10 \t Step: 60 \t Accuracy: 0.996999979019165\n","Testing\n","Test Accuracy: 0.997\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.datasets import mnist\n","import torch.nn as nn\n","\n","(train_images, train_labels),(test_images, test_labels)=mnist.load_data()\n","print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n","train_images=train_images.reshape([-1,784]).astype('float32')/255.0\n","test_images=test_images.reshape([-1,784]).astype('float32')/255.0\n","train_labels=tf.one_hot(train_labels, depth=10) # size 10짜리 one-hot encoding 만듦\n","test_labels=tf.one_hot(test_labels, depth=10)\n","batch_size=1000\n","train_data=tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n","train_data=train_data.repeat().shuffle(5000).batch(batch_size) # 데이터를 shuffle 하여야 고른 분포를 가짐\n","\n","nH1=512\n","nH2=512\n","nH3=512\n","nH4=512\n","nH5=512\n","\n","class class_SoftmaxNN_LC(nn.Module): # class 는 항상 pytorch로 만들었기 때문에\n","  def __init__(self):\n","    self.W_1=tf.Variable(tf.random.normal([784,nH1]))\n","    self.b_1=tf.Variable(tf.random.normal([nH1]))\n","    self.W_2=tf.Variable(tf.random.normal([nH1,nH2]))\n","    self.b_2=tf.Variable(tf.random.normal([nH2]))\n","    self.W_3=tf.Variable(tf.random.normal([nH2,nH3]))\n","    self.b_3=tf.Variable(tf.random.normal([nH3]))\n","    self.W_4=tf.Variable(tf.random.normal([nH3,nH4]))\n","    self.b_4=tf.Variable(tf.random.normal([nH4]))\n","    self.W_5=tf.Variable(tf.random.normal([nH4,nH5]))\n","    self.b_5=tf.Variable(tf.random.normal([nH5]))\n","    self.W_o=tf.Variable(tf.random.normal([nH5,10]))\n","    self.b_o=tf.Variable(tf.random.normal([10]))\n","  def __call__(self,x):\n","    H_1=tf.nn.relu(tf.matmul(x,self.W_1)+self.b_1)\n","    H_2=tf.nn.relu(tf.matmul(H_1,self.W_2)+self.b_2)\n","    H_3=tf.nn.relu(tf.matmul(H_2,self.W_3)+self.b_3)\n","    H_4=tf.nn.relu(tf.matmul(H_3,self.W_4)+self.b_4)\n","    H_5=tf.nn.relu(tf.matmul(H_4,self.W_5)+self.b_5)\n","    Out=tf.matmul(H_5,self.W_o)+self.b_o\n","    return Out\n","\n","model_SoftmaxNN_LC=class_SoftmaxNN_LC()\n","optimizer=tf.optimizers.Adam(0.01)\n","\n","def cost_Softmax(x,y):\n","  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x,labels=y))\n","\n","def train_optimization(MNIST_images,MNIST_labels):\n","  with tf.GradientTape() as g:\n","    prediction=model_SoftmaxNN_LC(MNIST_images)\n","    cost=cost_Softmax(prediction,MNIST_labels)\n","  gradients=g.gradient(cost,vars(model_SoftmaxNN_LC).values())\n","  tf.optimizers.Adam(0.01).apply_gradients(zip(gradients,vars(model_SoftmaxNN_LC).values()))\n","\n","for epoch in range(1,11):\n","  for step, (batch_images,batch_labels) in enumerate(train_data.take(60),1):\n","    train_optimization(batch_images,batch_labels)\n","    if step%10==0:\n","      pred=tf.nn.softmax(model_SoftmaxNN_LC(batch_images))\n","      accuracy=tf.reduce_mean(tf.cast(tf.equal(pred,batch_labels),tf.float32))\n","      print('Epoch: {} \\t Step: {} \\t Accuracy: {}'.format(epoch,step,accuracy))\n","\n","print('Testing')\n","test_model=tf.nn.softmax(model_SoftmaxNN_LC(test_images))\n","test_accuracy=tf.reduce_mean(tf.cast(tf.equal(pred,batch_labels),tf.float32))\n","print('Test Accuracy:', test_accuracy.numpy())"]}]}