{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XOR(tensor flow 2.5).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMTUa6NBTdwQ9YmwcxeBiDx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qCYtWb-JN0sB","executionInfo":{"status":"ok","timestamp":1635296651623,"user_tz":-540,"elapsed":102221,"user":{"displayName":"박지훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6-Ag58_xLqlUgLlxTrvTfpPDdLrq6GdSW3nh4Kw=s64","userId":"08329669712993464717"}},"outputId":"1aaaee46-0fb3-4380-8c86-e1d8d44c971e"},"source":["import tensorflow as tf\n","import numpy as np\n","x_train = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n","y_train = np.array([[0], [1], [1], [0]], dtype=np.float32)\n","W_h = tf.Variable(tf.random.normal([2,3]))\n","b_h = tf.Variable(tf.random.normal([3]))\n","W_o = tf.Variable(tf.random.normal([3,1]))\n","b_o = tf.Variable(tf.random.normal([1]))\n","\n","def model_BinaryClassification(x):\n","  H1=tf.sigmoid(tf.matmul(x,W_h)+b_h)\n","  return tf.sigmoid(tf.matmul(H1,W_o)+b_o)\n","\n","def cost_BinaryClassification(model_x):\n","\treturn tf.reduce_mean((-1)*y_train*tf.math.log(model_x)+(-1)*(1.-y_train)*tf.math.log(1.-model_x))\n","\n","def train_optimization(x):\n","\twith tf.GradientTape() as g:\n","\t\tmodel = model_BinaryClassification(x)\n","\t\tcost = cost_BinaryClassification(model)\n","\tgradients = g.gradient(cost,[W_h,b_h,W_o,b_o])\n","\ttf.optimizers.SGD(0.1).apply_gradients(zip(gradients,[W_h,b_h,W_o,b_o]))\n","\n","for step in range(20001):\n","  train_optimization(x_train)\n","  if step % 100 == 0:\n","      pred = model_BinaryClassification(x_train)\n","      loss = cost_BinaryClassification(pred)\n","      prediction = tf.cast(pred > 0.5, dtype=tf.float32)\n","      accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_train), dtype=tf.float32))\n","      print(\"Step: {},\\t Accurray: {},\\t Loss: {}\".format(step,accuracy.numpy().flatten(),loss))\n","\n","print('='*100)\n","model_test=model_BinaryClassification(x_train)\n","print(model_test.numpy())\n","print('='*100)\n","\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Step: 0,\t Accurray: [0.5],\t Loss: 0.9071008563041687\n","Step: 100,\t Accurray: [0.5],\t Loss: 0.6916213631629944\n","Step: 200,\t Accurray: [0.75],\t Loss: 0.689469575881958\n","Step: 300,\t Accurray: [0.75],\t Loss: 0.6870840787887573\n","Step: 400,\t Accurray: [0.5],\t Loss: 0.6840856075286865\n","Step: 500,\t Accurray: [0.5],\t Loss: 0.6801735162734985\n","Step: 600,\t Accurray: [0.75],\t Loss: 0.6751056909561157\n","Step: 700,\t Accurray: [0.75],\t Loss: 0.6686855554580688\n","Step: 800,\t Accurray: [0.75],\t Loss: 0.6607154011726379\n","Step: 900,\t Accurray: [0.75],\t Loss: 0.6509655714035034\n","Step: 1000,\t Accurray: [0.75],\t Loss: 0.6392324566841125\n","Step: 1100,\t Accurray: [0.75],\t Loss: 0.625481903553009\n","Step: 1200,\t Accurray: [0.75],\t Loss: 0.6099509000778198\n","Step: 1300,\t Accurray: [0.75],\t Loss: 0.5930972695350647\n","Step: 1400,\t Accurray: [0.75],\t Loss: 0.5754656791687012\n","Step: 1500,\t Accurray: [0.75],\t Loss: 0.5575758814811707\n","Step: 1600,\t Accurray: [0.75],\t Loss: 0.5398530960083008\n","Step: 1700,\t Accurray: [0.75],\t Loss: 0.5225839614868164\n","Step: 1800,\t Accurray: [0.75],\t Loss: 0.5058828592300415\n","Step: 1900,\t Accurray: [0.75],\t Loss: 0.4896622896194458\n","Step: 2000,\t Accurray: [0.75],\t Loss: 0.47360336780548096\n","Step: 2100,\t Accurray: [0.75],\t Loss: 0.45716387033462524\n","Step: 2200,\t Accurray: [0.75],\t Loss: 0.43969476222991943\n","Step: 2300,\t Accurray: [0.75],\t Loss: 0.42066097259521484\n","Step: 2400,\t Accurray: [0.75],\t Loss: 0.3997657597064972\n","Step: 2500,\t Accurray: [1.],\t Loss: 0.3769056797027588\n","Step: 2600,\t Accurray: [1.],\t Loss: 0.3522271513938904\n","Step: 2700,\t Accurray: [1.],\t Loss: 0.3263060450553894\n","Step: 2800,\t Accurray: [1.],\t Loss: 0.30010664463043213\n","Step: 2900,\t Accurray: [1.],\t Loss: 0.2746639847755432\n","Step: 3000,\t Accurray: [1.],\t Loss: 0.25077512860298157\n","Step: 3100,\t Accurray: [1.],\t Loss: 0.2288874089717865\n","Step: 3200,\t Accurray: [1.],\t Loss: 0.20914986729621887\n","Step: 3300,\t Accurray: [1.],\t Loss: 0.1915172040462494\n","Step: 3400,\t Accurray: [1.],\t Loss: 0.17584228515625\n","Step: 3500,\t Accurray: [1.],\t Loss: 0.16193532943725586\n","Step: 3600,\t Accurray: [1.],\t Loss: 0.1495979130268097\n","Step: 3700,\t Accurray: [1.],\t Loss: 0.13864073157310486\n","Step: 3800,\t Accurray: [1.],\t Loss: 0.12889045476913452\n","Step: 3900,\t Accurray: [1.],\t Loss: 0.12019281089305878\n","Step: 4000,\t Accurray: [1.],\t Loss: 0.11241252720355988\n","Step: 4100,\t Accurray: [1.],\t Loss: 0.10543198883533478\n","Step: 4200,\t Accurray: [1.],\t Loss: 0.09914973378181458\n","Step: 4300,\t Accurray: [1.],\t Loss: 0.09347816556692123\n","Step: 4400,\t Accurray: [1.],\t Loss: 0.08834189176559448\n","Step: 4500,\t Accurray: [1.],\t Loss: 0.08367612957954407\n","Step: 4600,\t Accurray: [1.],\t Loss: 0.07942507416009903\n","Step: 4700,\t Accurray: [1.],\t Loss: 0.07554057240486145\n","Step: 4800,\t Accurray: [1.],\t Loss: 0.07198105752468109\n","Step: 4900,\t Accurray: [1.],\t Loss: 0.0687105730175972\n","Step: 5000,\t Accurray: [1.],\t Loss: 0.06569778174161911\n","Step: 5100,\t Accurray: [1.],\t Loss: 0.06291557848453522\n","Step: 5200,\t Accurray: [1.],\t Loss: 0.060340169817209244\n","Step: 5300,\t Accurray: [1.],\t Loss: 0.05795068293809891\n","Step: 5400,\t Accurray: [1.],\t Loss: 0.055728983134031296\n","Step: 5500,\t Accurray: [1.],\t Loss: 0.05365893617272377\n","Step: 5600,\t Accurray: [1.],\t Loss: 0.051726438105106354\n","Step: 5700,\t Accurray: [1.],\t Loss: 0.04991883039474487\n","Step: 5800,\t Accurray: [1.],\t Loss: 0.04822508245706558\n","Step: 5900,\t Accurray: [1.],\t Loss: 0.04663524031639099\n","Step: 6000,\t Accurray: [1.],\t Loss: 0.045140527188777924\n","Step: 6100,\t Accurray: [1.],\t Loss: 0.04373297840356827\n","Step: 6200,\t Accurray: [1.],\t Loss: 0.04240552708506584\n","Step: 6300,\t Accurray: [1.],\t Loss: 0.041151776909828186\n","Step: 6400,\t Accurray: [1.],\t Loss: 0.03996611386537552\n","Step: 6500,\t Accurray: [1.],\t Loss: 0.03884325921535492\n","Step: 6600,\t Accurray: [1.],\t Loss: 0.037778496742248535\n","Step: 6700,\t Accurray: [1.],\t Loss: 0.036767780780792236\n","Step: 6800,\t Accurray: [1.],\t Loss: 0.035807035863399506\n","Step: 6900,\t Accurray: [1.],\t Loss: 0.03489294275641441\n","Step: 7000,\t Accurray: [1.],\t Loss: 0.03402221202850342\n","Step: 7100,\t Accurray: [1.],\t Loss: 0.03319193795323372\n","Step: 7200,\t Accurray: [1.],\t Loss: 0.03239942342042923\n","Step: 7300,\t Accurray: [1.],\t Loss: 0.031642306596040726\n","Step: 7400,\t Accurray: [1.],\t Loss: 0.030918218195438385\n","Step: 7500,\t Accurray: [1.],\t Loss: 0.03022526204586029\n","Step: 7600,\t Accurray: [1.],\t Loss: 0.029561489820480347\n","Step: 7700,\t Accurray: [1.],\t Loss: 0.028925083577632904\n","Step: 7800,\t Accurray: [1.],\t Loss: 0.02831444889307022\n","Step: 7900,\t Accurray: [1.],\t Loss: 0.027728091925382614\n","Step: 8000,\t Accurray: [1.],\t Loss: 0.027164682745933533\n","Step: 8100,\t Accurray: [1.],\t Loss: 0.026622895151376724\n","Step: 8200,\t Accurray: [1.],\t Loss: 0.02610158734023571\n","Step: 8300,\t Accurray: [1.],\t Loss: 0.02559952437877655\n","Step: 8400,\t Accurray: [1.],\t Loss: 0.02511586993932724\n","Step: 8500,\t Accurray: [1.],\t Loss: 0.024649420753121376\n","Step: 8600,\t Accurray: [1.],\t Loss: 0.024199485778808594\n","Step: 8700,\t Accurray: [1.],\t Loss: 0.02376519702374935\n","Step: 8800,\t Accurray: [1.],\t Loss: 0.02334568090736866\n","Step: 8900,\t Accurray: [1.],\t Loss: 0.022940315306186676\n","Step: 9000,\t Accurray: [1.],\t Loss: 0.02254830300807953\n","Step: 9100,\t Accurray: [1.],\t Loss: 0.022169098258018494\n","Step: 9200,\t Accurray: [1.],\t Loss: 0.0218021459877491\n","Step: 9300,\t Accurray: [1.],\t Loss: 0.02144671231508255\n","Step: 9400,\t Accurray: [1.],\t Loss: 0.021102383732795715\n","Step: 9500,\t Accurray: [1.],\t Loss: 0.020768623799085617\n","Step: 9600,\t Accurray: [1.],\t Loss: 0.0204449649900198\n","Step: 9700,\t Accurray: [1.],\t Loss: 0.020130962133407593\n","Step: 9800,\t Accurray: [1.],\t Loss: 0.019826240837574005\n","Step: 9900,\t Accurray: [1.],\t Loss: 0.019530359655618668\n","Step: 10000,\t Accurray: [1.],\t Loss: 0.019242942333221436\n","Step: 10100,\t Accurray: [1.],\t Loss: 0.01896364614367485\n","Step: 10200,\t Accurray: [1.],\t Loss: 0.018692173063755035\n","Step: 10300,\t Accurray: [1.],\t Loss: 0.018428178504109383\n","Step: 10400,\t Accurray: [1.],\t Loss: 0.018171317875385284\n","Step: 10500,\t Accurray: [1.],\t Loss: 0.01792137138545513\n","Step: 10600,\t Accurray: [1.],\t Loss: 0.017678027972579002\n","Step: 10700,\t Accurray: [1.],\t Loss: 0.017441069707274437\n","Step: 10800,\t Accurray: [1.],\t Loss: 0.017210276797413826\n","Step: 10900,\t Accurray: [1.],\t Loss: 0.016985278576612473\n","Step: 11000,\t Accurray: [1.],\t Loss: 0.016766037791967392\n","Step: 11100,\t Accurray: [1.],\t Loss: 0.016552217304706573\n","Step: 11200,\t Accurray: [1.],\t Loss: 0.016343720257282257\n","Step: 11300,\t Accurray: [1.],\t Loss: 0.0161402840167284\n","Step: 11400,\t Accurray: [1.],\t Loss: 0.015941722318530083\n","Step: 11500,\t Accurray: [1.],\t Loss: 0.01574789360165596\n","Step: 11600,\t Accurray: [1.],\t Loss: 0.015558584593236446\n","Step: 11700,\t Accurray: [1.],\t Loss: 0.015373758040368557\n","Step: 11800,\t Accurray: [1.],\t Loss: 0.01519313920289278\n","Step: 11900,\t Accurray: [1.],\t Loss: 0.015016619116067886\n","Step: 12000,\t Accurray: [1.],\t Loss: 0.014844116754829884\n","Step: 12100,\t Accurray: [1.],\t Loss: 0.014675421640276909\n","Step: 12200,\t Accurray: [1.],\t Loss: 0.014510449022054672\n","Step: 12300,\t Accurray: [1.],\t Loss: 0.014349080622196198\n","Step: 12400,\t Accurray: [1.],\t Loss: 0.014191189780831337\n","Step: 12500,\t Accurray: [1.],\t Loss: 0.014036761596798897\n","Step: 12600,\t Accurray: [1.],\t Loss: 0.01388547196984291\n","Step: 12700,\t Accurray: [1.],\t Loss: 0.013737382367253304\n","Step: 12800,\t Accurray: [1.],\t Loss: 0.0135923707857728\n","Step: 12900,\t Accurray: [1.],\t Loss: 0.013450326398015022\n","Step: 13000,\t Accurray: [1.],\t Loss: 0.013311203569173813\n","Step: 13100,\t Accurray: [1.],\t Loss: 0.013174849562346935\n","Step: 13200,\t Accurray: [1.],\t Loss: 0.013041216880083084\n","Step: 13300,\t Accurray: [1.],\t Loss: 0.012910228222608566\n","Step: 13400,\t Accurray: [1.],\t Loss: 0.012781823053956032\n","Step: 13500,\t Accurray: [1.],\t Loss: 0.012655876576900482\n","Step: 13600,\t Accurray: [1.],\t Loss: 0.01253235898911953\n","Step: 13700,\t Accurray: [1.],\t Loss: 0.012411179021000862\n","Step: 13800,\t Accurray: [1.],\t Loss: 0.012292332015931606\n","Step: 13900,\t Accurray: [1.],\t Loss: 0.01217564009130001\n","Step: 14000,\t Accurray: [1.],\t Loss: 0.012061113491654396\n","Step: 14100,\t Accurray: [1.],\t Loss: 0.011948708444833755\n","Step: 14200,\t Accurray: [1.],\t Loss: 0.011838346719741821\n","Step: 14300,\t Accurray: [1.],\t Loss: 0.011729998514056206\n","Step: 14400,\t Accurray: [1.],\t Loss: 0.01162354089319706\n","Step: 14500,\t Accurray: [1.],\t Loss: 0.011518973857164383\n","Step: 14600,\t Accurray: [1.],\t Loss: 0.011416265740990639\n","Step: 14700,\t Accurray: [1.],\t Loss: 0.011315355077385902\n","Step: 14800,\t Accurray: [1.],\t Loss: 0.011216167360544205\n","Step: 14900,\t Accurray: [1.],\t Loss: 0.011118684895336628\n","Step: 15000,\t Accurray: [1.],\t Loss: 0.011022817343473434\n","Step: 15100,\t Accurray: [1.],\t Loss: 0.010928639210760593\n","Step: 15200,\t Accurray: [1.],\t Loss: 0.010835983790457249\n","Step: 15300,\t Accurray: [1.],\t Loss: 0.01074486505240202\n","Step: 15400,\t Accurray: [1.],\t Loss: 0.010655269026756287\n","Step: 15500,\t Accurray: [1.],\t Loss: 0.01056713331490755\n","Step: 15600,\t Accurray: [1.],\t Loss: 0.010480428114533424\n","Step: 15700,\t Accurray: [1.],\t Loss: 0.010395091027021408\n","Step: 15800,\t Accurray: [1.],\t Loss: 0.010311122983694077\n","Step: 15900,\t Accurray: [1.],\t Loss: 0.010228493250906467\n","Step: 16000,\t Accurray: [1.],\t Loss: 0.010147156193852425\n","Step: 16100,\t Accurray: [1.],\t Loss: 0.010067064315080643\n","Step: 16200,\t Accurray: [1.],\t Loss: 0.009988220408558846\n","Step: 16300,\t Accurray: [1.],\t Loss: 0.009910620748996735\n","Step: 16400,\t Accurray: [1.],\t Loss: 0.009834147989749908\n","Step: 16500,\t Accurray: [1.],\t Loss: 0.009758902713656425\n","Step: 16600,\t Accurray: [1.],\t Loss: 0.009684722870588303\n","Step: 16700,\t Accurray: [1.],\t Loss: 0.009611651301383972\n","Step: 16800,\t Accurray: [1.],\t Loss: 0.009539702907204628\n","Step: 16900,\t Accurray: [1.],\t Loss: 0.009468787349760532\n","Step: 17000,\t Accurray: [1.],\t Loss: 0.00939896423369646\n","Step: 17100,\t Accurray: [1.],\t Loss: 0.009330022148787975\n","Step: 17200,\t Accurray: [1.],\t Loss: 0.009262172505259514\n","Step: 17300,\t Accurray: [1.],\t Loss: 0.009195264428853989\n","Step: 17400,\t Accurray: [1.],\t Loss: 0.009129296988248825\n","Step: 17500,\t Accurray: [1.],\t Loss: 0.009064254350960255\n","Step: 17600,\t Accurray: [1.],\t Loss: 0.009000137448310852\n","Step: 17700,\t Accurray: [1.],\t Loss: 0.008936868980526924\n","Step: 17800,\t Accurray: [1.],\t Loss: 0.008874496445059776\n","Step: 17900,\t Accurray: [1.],\t Loss: 0.008813047781586647\n","Step: 18000,\t Accurray: [1.],\t Loss: 0.00875235814601183\n","Step: 18100,\t Accurray: [1.],\t Loss: 0.008692502044141293\n","Step: 18200,\t Accurray: [1.],\t Loss: 0.008633449673652649\n","Step: 18300,\t Accurray: [1.],\t Loss: 0.00857524573802948\n","Step: 18400,\t Accurray: [1.],\t Loss: 0.008517754264175892\n","Step: 18500,\t Accurray: [1.],\t Loss: 0.008460991084575653\n","Step: 18600,\t Accurray: [1.],\t Loss: 0.008405028842389584\n","Step: 18700,\t Accurray: [1.],\t Loss: 0.008349734358489513\n","Step: 18800,\t Accurray: [1.],\t Loss: 0.008295197039842606\n","Step: 18900,\t Accurray: [1.],\t Loss: 0.008241387084126472\n","Step: 19000,\t Accurray: [1.],\t Loss: 0.008188243955373764\n","Step: 19100,\t Accurray: [1.],\t Loss: 0.008135691285133362\n","Step: 19200,\t Accurray: [1.],\t Loss: 0.008083909749984741\n","Step: 19300,\t Accurray: [1.],\t Loss: 0.008032764308154583\n","Step: 19400,\t Accurray: [1.],\t Loss: 0.007982240989804268\n","Step: 19500,\t Accurray: [1.],\t Loss: 0.00793230626732111\n","Step: 19600,\t Accurray: [1.],\t Loss: 0.007883038371801376\n","Step: 19700,\t Accurray: [1.],\t Loss: 0.007834346033632755\n","Step: 19800,\t Accurray: [1.],\t Loss: 0.007786243222653866\n","Step: 19900,\t Accurray: [1.],\t Loss: 0.007738715969026089\n","Step: 20000,\t Accurray: [1.],\t Loss: 0.007691763341426849\n","====================================================================================================\n","[[7.34299421e-04]\n"," [9.91701245e-01]\n"," [9.91413772e-01]\n"," [1.29906535e-02]]\n","====================================================================================================\n"]}]},{"cell_type":"code","metadata":{"id":"b1D5wAVaQxNl"},"source":[""],"execution_count":null,"outputs":[]}]}